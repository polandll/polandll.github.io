<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Dynamo :: Apache Cassandra Documentation</title>
    <meta name="generator" content="Antora 2.3.1">
    <link rel="stylesheet" href="../../../../_/css/site.css">
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="../../../..">Apache Cassandra Documentation&nbsp;&nbsp;&nbsp;<img src="../../../../_/img/cassandra_logo.png"></a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="#">Home</a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Download</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Latest</a>
            <a class="navbar-item" href="#">3.x</a>
            <a class="navbar-item" href="#">3.0</a>
            <a class="navbar-item" href="#">2.1</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Documentation</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Latest</a>
            <a class="navbar-item" href="#">3.x</a>
            <a class="navbar-item" href="#">3.0</a>
            <a class="navbar-item" href="#">2.1</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Community</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Developer Mailing List</a>
            <a class="navbar-item" href="#">User Mailing List</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Blog</a>
	</div>
          </span>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="ASCIIDOC_POC" data-version="4.0">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../../index.html">ASCIIDOC_POC</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../../index.html">Cassandra Documentation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../glossary.html">Glossary</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../bugs.html">How to report bugs</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../../contactus.html">Contact us</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Cassandra</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="overview.html">Overview</a>
  </li>
  <li class="nav-item is-current-page" data-depth="3">
    <a class="nav-link" href="dynamo.html">Dynamo</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="storage_engine.html">Storage engine</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="guarantees.html">Guarantees</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../new/index.html">What&#8217;s new</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../getting_started/index.html">Getting Started</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../getting_started/installing.html">Installing Cassandra</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../getting_started/configuring.html">Configuring Cassandra</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../getting_started/querying.html">Inserting and querying</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../getting_started/drivers.html">Client drivers</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../getting_started/production.html">Production recommendations</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../data_modeling/index.html">Data Modeling</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="#cql/index.adoc{">CQL</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/changes.html">Changes</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/ddl.html">DDL</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/dml.html">DML</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/types.html">Data types</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/operators.html">Operators</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/definitions.html">Definitions</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/indexes.html">Secondary indexes</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/mvs.html">Materialized views</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/functions.html">Functions</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/json.html">JSON</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/security.html">Security</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/triggers.html">Triggers</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="../cql/appendices.html">Appendices</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../configuration/index.html">Configuration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../operating/index.html">Operating</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../tools/index.html">Tools</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../development/index.html">Development</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../plugins/index.html">Plug-ins</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../troubleshooting/index.html">Troubleshooting</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../faq/index.html">FAQ</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">ASCIIDOC_POC</span>
    <span class="version">4.0</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <span class="title">ASCIIDOC_POC</span>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../../index.html">4.0</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../../index.html">ASCIIDOC_POC</a></li>
    <li>Cassandra</li>
    <li><a href="index.html">Architecture</a></li>
    <li><a href="dynamo.html">Dynamo</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="file:///Users/lorina.poland/CLONES/cassandra-examples/rst-to-asciidoc-tests/ASCIIDOC/modules/cassandra/pages/architecture/dynamo.adoc">Edit this Page</a></div>
</div>
  <div class="content">
<article class="doc">
<h1 class="page">Dynamo</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Apache Cassandra relies on a number of techniques from Amazon&#8217;s
<a href="http://courses.cse.tamu.edu/caverlee/csce438/readings/dynamo-paper.pdf">Dynamo</a>
distributed storage key-value system. Each node in the Dynamo system has
three main components:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Request coordination over a partitioned dataset</p>
</li>
<li>
<p>Ring membership and failure detection</p>
</li>
<li>
<p>A local persistence (storage) engine</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Cassandra primarily draws from the first two clustering components,
while using a storage engine based on a Log Structured Merge Tree
(<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.44.2782&amp;rep=rep1&amp;type=pdf">LSM</a>).
In particular, Cassandra relies on Dynamo style:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Dataset partitioning using consistent hashing</p>
</li>
<li>
<p>Multi-master replication using versioned data and tunable consistency</p>
</li>
<li>
<p>Distributed cluster membership and failure detection via a gossip
protocol</p>
</li>
<li>
<p>Incremental scale-out on commodity hardware</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Cassandra was designed this way to meet large-scale (PiB+)
business-critical storage requirements. In particular, as applications
demanded full global replication of petabyte scale datasets along with
always available low-latency reads and writes, it became imperative to
design a new kind of database model as the relational database systems
of the time struggled to meet the new requirements of global scale
applications.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="dataset-partitioning-consistent-hashing"><a class="anchor" href="#dataset-partitioning-consistent-hashing"></a>Dataset Partitioning: Consistent Hashing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Cassandra achieves horizontal scalability by
<a href="https://en.wikipedia.org/wiki/Partition_(database)">partitioning</a> all
data stored in the system using a hash function. Each partition is
replicated to multiple physical nodes, often across failure domains such
as racks and even datacenters. As every replica can independently accept
mutations to every key that it owns, every key must be versioned. Unlike
in the original Dynamo paper where deterministic versions and vector
clocks were used to reconcile concurrent updates to a key, Cassandra
uses a simpler last write wins model where every mutation is timestamped
(including deletes) and then the latest version of data is the "winning"
value. Formally speaking, Cassandra uses a Last-Write-Wins Element-Set
conflict-free replicated data type for each CQL row (a.k.a
<a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#LWW-Element-Set_(Last-Write-Wins-Element-Set)">LWW-Element-Set
CRDT</a>) to resolve conflicting mutations on replica sets.</p>
</div>
<div class="quoteblock">
<blockquote>

</blockquote>
</div>
<div class="sect2">
<h3 id="consistent-hashing-using-a-token-ring"><a class="anchor" href="#consistent-hashing-using-a-token-ring"></a>Consistent Hashing using a Token Ring</h3>
<div class="paragraph">
<p>Cassandra partitions data over storage nodes using a special form of
hashing called
<a href="https://en.wikipedia.org/wiki/Consistent_hashing">consistent hashing</a>. In
naive data hashing, you typically allocate keys to buckets by taking a
hash of the key modulo the number of buckets. For example, if you want
to distribute data to 100 nodes using naive hashing you might assign
every node to a bucket between 0 and 100, hash the input key modulo 100,
and store the data on the associated bucket. In this naive scheme,
however, adding a single node might invalidate almost all of the
mappings.</p>
</div>
<div class="paragraph">
<p>Cassandra instead maps every node to one or more tokens on a continuous
hash ring, and defines ownership by hashing a key onto the ring and then
"walking" the ring in one direction, similar to the
<a href="https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf">Chord</a>
algorithm. The main difference of consistent hashing to naive data
hashing is that when the number of nodes (buckets) to hash into changes,
consistent hashing only has to move a small fraction of the keys.</p>
</div>
<div class="paragraph">
<p>For example, if we have an eight node cluster with evenly spaced tokens,
and a replication factor (RF) of 3, then to find the owning nodes for a
key we first hash that key to generate a token (which is just the hash
of the key), and then we "walk" the ring in a clockwise fashion until we
encounter three distinct nodes, at which point we have found all the
replicas of that key. This example of an eight node cluster with
<span class="title-ref">RF=3</span> can be visualized as follows:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../_images/ring.svg" alt="image">
</div>
</div>
<div class="paragraph">
<p>You can see that in a Dynamo like system, ranges of keys, also known as
<strong>token ranges</strong>, map to the same physical set of nodes. In this example,
all keys that fall in the token range excluding token 1 and including
token 2 (<span class="title-ref">range(t1, t2]</span>) are stored on nodes 2, 3 and 4.</p>
</div>
</div>
<div class="sect2">
<h3 id="multiple-tokens-per-physical-node-a-k-a-vnodes"><a class="anchor" href="#multiple-tokens-per-physical-node-a-k-a-vnodes"></a>Multiple Tokens per Physical Node (a.k.a. <span class="title-ref">vnodes</span>)</h3>
<div class="paragraph">
<p>Simple single token consistent hashing works well if you have many
physical nodes to spread data over, but with evenly spaced tokens and a
small number of physical nodes, incremental scaling (adding just a few
nodes of capacity) is difficult because there are no token selections
for new nodes that can leave the ring balanced. Cassandra seeks to avoid
token imbalance because uneven token ranges lead to uneven request load.
For example, in the previous example there is no way to add a ninth
token without causing imbalance; instead we would have to insert <code>8</code>
tokens in the midpoints of the existing ranges.</p>
</div>
<div class="paragraph">
<p>The Dynamo paper advocates for the use of "virtual nodes" to solve this
imbalance problem. Virtual nodes solve the problem by assigning multiple
tokens in the token ring to each physical node. By allowing a single
physical node to take multiple positions in the ring, we can make small
clusters look larger and therefore even with a single physical node
addition we can make it look like we added many more nodes, effectively
taking many smaller pieces of data from more ring neighbors when we add
even a single node.</p>
</div>
<div class="paragraph">
<p>Cassandra introduces some nomenclature to handle these concepts:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Token</strong>: A single position on the <span class="title-ref">dynamo</span> style hash
ring.</p>
</li>
<li>
<p><strong>Endpoint</strong>: A single physical IP and port on the network.</p>
</li>
<li>
<p><strong>Host ID</strong>: A unique identifier for a single "physical" node, usually
present at one <span class="title-ref">Endpoint</span> and containing one or more
<span class="title-ref">Tokens</span>.</p>
</li>
<li>
<p><strong>Virtual Node</strong> (or <strong>vnode</strong>): A <span class="title-ref">Token</span> on the hash ring
owned by the same physical node, one with the same <span class="title-ref">Host
ID</span>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The mapping of <strong>Tokens</strong> to <strong>Endpoints</strong> gives rise to the <strong>Token Map</strong>
where Cassandra keeps track of what ring positions map to which physical
endpoints. For example, in the following figure we can represent an
eight node cluster using only four physical nodes by assigning two
tokens to every node:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="../_images/vnodes.svg" alt="image">
</div>
</div>
<div class="paragraph">
<p>Multiple tokens per physical node provide the following benefits:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>When a new node is added it accepts approximately equal amounts of
data from other nodes in the ring, resulting in equal distribution of
data across the cluster.</p>
</li>
<li>
<p>When a node is decommissioned, it loses data roughly equally to other
members of the ring, again keeping equal distribution of data across the
cluster.</p>
</li>
<li>
<p>If a node becomes unavailable, query load (especially token aware
query load), is evenly distributed across many other nodes.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Multiple tokens, however, can also have disadvantages:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Every token introduces up to <code>2 * (RF - 1)</code> additional neighbors on
the token ring, which means that there are more combinations of node
failures where we lose availability for a portion of the token ring. The
more tokens you have,
<a href="https://jolynch.github.io/pdf/cassandra-availability-virtual.pdf">the
higher the probability of an outage</a>.</p>
</li>
<li>
<p>Cluster-wide maintenance operations are often slowed. For example, as
the number of tokens per node is increased, the number of discrete
repair operations the cluster must do also increases.</p>
</li>
<li>
<p>Performance of operations that span token ranges could be affected.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Note that in Cassandra <code>2.x</code>, the only token allocation algorithm
available was picking random tokens, which meant that to keep balance
the default number of tokens per node had to be quite high, at <code>256</code>.
This had the effect of coupling many physical endpoints together,
increasing the risk of unavailability. That is why in <code>3.x +</code> the new
deterministic token allocator was added which intelligently picks tokens
such that the ring is optimally balanced while requiring a much lower
number of tokens per physical node.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="multi-master-replication-versioned-data-and-tunable-consistency"><a class="anchor" href="#multi-master-replication-versioned-data-and-tunable-consistency"></a>Multi-master Replication: Versioned Data and Tunable Consistency</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Cassandra replicates every partition of data to many nodes across the
cluster to maintain high availability and durability. When a mutation
occurs, the coordinator hashes the partition key to determine the token
range the data belongs to and then replicates the mutation to the
replicas of that data according to the
<code>Replication Strategy &lt;replication-strategy&gt;</code>.</p>
</div>
<div class="paragraph">
<p>All replication strategies have the notion of a <strong>replication factor</strong>
(<code>RF</code>), which indicates to Cassandra how many copies of the partition
should exist. For example with a <code>RF=3</code> keyspace, the data will be
written to three distinct <strong>replicas</strong>. Replicas are always chosen such
that they are distinct physical nodes which is achieved by skipping
virtual nodes if needed. Replication strategies may also choose to skip
nodes present in the same failure domain such as racks or datacenters so
that Cassandra clusters can tolerate failures of whole racks and even
datacenters of nodes.</p>
</div>
<div class="sect2">
<h3 id="replication-strategy"><a class="anchor" href="#replication-strategy"></a>Replication Strategy</h3>
<div class="paragraph">
<p>Cassandra supports pluggable <strong>replication strategies</strong>, which determine
which physical nodes act as replicas for a given token range. Every
keyspace of data has its own replication strategy. All production
deployments should use the <code>network-topology-strategy</code> while the
<code>simple-strategy</code> replication strategy is useful only for testing
clusters where you do not yet know the datacenter layout of the cluster.</p>
</div>
<div class="sect3">
<h4 id="network-topology-strategy"><a class="anchor" href="#network-topology-strategy"></a><code>NetworkTopologyStrategy</code></h4>
<div class="paragraph">
<p><code>NetworkTopologyStrategy</code> allows a replication factor to be specified
for each datacenter in the cluster. Even if your cluster only uses a
single datacenter, <code>NetworkTopologyStrategy</code> should be preferred over
<code>SimpleStrategy</code> to make it easier to add new physical or virtual
datacenters to the cluster later.</p>
</div>
<div class="paragraph">
<p>In addition to allowing the replication factor to be specified
individually by datacenter, <code>NetworkTopologyStrategy</code> also attempts to
choose replicas within a datacenter from different racks as specified by
the <code>Snitch &lt;snitch&gt;</code>. If the number of racks is greater than or equal
to the replication factor for the datacenter, each replica is guaranteed
to be chosen from a different rack. Otherwise, each rack will hold at
least one replica, but some racks may hold more than one. Note that this
rack-aware behavior has some potentially
<a href="https://issues.apache.org/jira/browse/CASSANDRA-3810">surprising
implications</a>. For example, if there are not an even number of nodes in
each rack, the data load on the smallest rack may be much higher.
Similarly, if a single node is bootstrapped into a brand new rack, it
will be considered a replica for the entire ring. For this reason, many
operators choose to configure all nodes in a single availability zone or
similar failure domain as a single "rack".</p>
</div>
</div>
<div class="sect3">
<h4 id="simple-strategy"><a class="anchor" href="#simple-strategy"></a><code>SimpleStrategy</code></h4>
<div class="paragraph">
<p><code>SimpleStrategy</code> allows a single integer <code>replication_factor</code> to be
defined. This determines the number of nodes that should contain a copy
of each row. For example, if <code>replication_factor</code> is 3, then three
different nodes should store a copy of each row.</p>
</div>
<div class="paragraph">
<p><code>SimpleStrategy</code> treats all nodes identically, ignoring any configured
datacenters or racks. To determine the replicas for a token range,
Cassandra iterates through the tokens in the ring, starting with the
token range of interest. For each token, it checks whether the owning
node has been added to the set of replicas, and if it has not, it is
added to the set. This process continues until <code>replication_factor</code>
distinct nodes have been added to the set of replicas.</p>
</div>
</div>
<div class="sect3">
<h4 id="transient-replication"><a class="anchor" href="#transient-replication"></a>Transient Replication</h4>
<div class="paragraph">
<p>Transient replication is an experimental feature in Cassandra 4.0 not
present in the original Dynamo paper. It allows you to configure a
subset of replicas to only replicate data that hasn&#8217;t been incrementally
repaired. This allows you to decouple data redundancy from availability.
For instance, if you have a keyspace replicated at rf 3, and alter it to
rf 5 with 2 transient replicas, you go from being able to tolerate one
failed replica to being able to tolerate two, without corresponding
increase in storage usage. This is because 3 nodes will replicate all
the data for a given token range, and the other 2 will only replicate
data that hasn&#8217;t been incrementally repaired.</p>
</div>
<div class="paragraph">
<p>To use transient replication, you first need to enable it in
<code>cassandra.yaml</code>. Once enabled, both <code>SimpleStrategy</code> and
<code>NetworkTopologyStrategy</code> can be configured to transiently replicate
data. You configure it by specifying replication factor as
<code>&lt;total_replicas&gt;/&lt;transient_replicas</code> Both <code>SimpleStrategy</code> and
<code>NetworkTopologyStrategy</code> support configuring transient replication.</p>
</div>
<div class="paragraph">
<p>Transiently replicated keyspaces only support tables created with
read_repair set to <code>NONE</code> and monotonic reads are not currently
supported. You also can&#8217;t use <code>LWT</code>, logged batches, or counters in 4.0.
You will possibly never be able to use materialized views with
transiently replicated keyspaces and probably never be able to use
secondary indices with them.</p>
</div>
<div class="paragraph">
<p>Transient replication is an experimental feature that may not be ready
for production use. The expected audience is experienced users of
Cassandra capable of fully validating a deployment of their particular
application. That means being able check that operations like reads,
writes, decommission, remove, rebuild, repair, and replace all work with
your queries, data, configuration, operational practices, and
availability requirements.</p>
</div>
<div class="paragraph">
<p>It is anticipated that <code>4.next</code> will support monotonic reads with
transient replication as well as LWT, logged batches, and counters.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="data-versioning"><a class="anchor" href="#data-versioning"></a>Data Versioning</h3>
<div class="paragraph">
<p>Cassandra uses mutation timestamp versioning to guarantee eventual
consistency of data. Specifically all mutations that enter the system do
so with a timestamp provided either from a client clock or, absent a
client provided timestamp, from the coordinator node&#8217;s clock. Updates
resolve according to the conflict resolution rule of last write wins.
Cassandra&#8217;s correctness does depend on these clocks, so make sure a
proper time synchronization process is running such as NTP.</p>
</div>
<div class="paragraph">
<p>Cassandra applies separate mutation timestamps to every column of every
row within a CQL partition. Rows are guaranteed to be unique by primary
key, and each column in a row resolve concurrent mutations according to
last-write-wins conflict resolution. This means that updates to
different primary keys within a partition can actually resolve without
conflict! Furthermore the CQL collection types such as maps and sets use
this same conflict free mechanism, meaning that concurrent updates to
maps and sets are guaranteed to resolve as well.</p>
</div>
<div class="sect3">
<h4 id="replica-synchronization"><a class="anchor" href="#replica-synchronization"></a>Replica Synchronization</h4>
<div class="paragraph">
<p>As replicas in Cassandra can accept mutations independently, it is
possible for some replicas to have newer data than others. Cassandra has
many best-effort techniques to drive convergence of replicas including
<span class="title-ref">Replica read repair &lt;read-repair&gt;</span> in the read path and
<span class="title-ref">Hinted handoff &lt;hints&gt;</span> in the write path.</p>
</div>
<div class="paragraph">
<p>These techniques are only best-effort, however, and to guarantee
eventual consistency Cassandra implements <span class="title-ref">anti-entropy
repair &lt;repair&gt;</span> where replicas calculate hierarchical hash-trees over
their datasets called <a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle
Trees</a> that can then be compared across replicas to identify mismatched
data. Like the original Dynamo paper Cassandra supports "full" repairs
where replicas hash their entire dataset, create Merkle trees, send them
to each other and sync any ranges that don&#8217;t match.</p>
</div>
<div class="paragraph">
<p>Unlike the original Dynamo paper, Cassandra also implements sub-range
repair and incremental repair. Sub-range repair allows Cassandra to
increase the resolution of the hash trees (potentially down to the
single partition level) by creating a larger number of trees that span
only a portion of the data range. Incremental repair allows Cassandra to
only repair the partitions that have changed since the last repair.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="tunable-consistency"><a class="anchor" href="#tunable-consistency"></a>Tunable Consistency</h3>
<div class="paragraph">
<p>Cassandra supports a per-operation tradeoff between consistency and
availability through <strong>Consistency Levels</strong>. Cassandra&#8217;s consistency
levels are a version of Dynamo&#8217;s <code>R + W &gt; N</code> consistency mechanism where
operators could configure the number of nodes that must participate in
reads (<code>R</code>) and writes (<code>W</code>) to be larger than the replication factor
(<code>N</code>). In Cassandra, you instead choose from a menu of common
consistency levels which allow the operator to pick <code>R</code> and <code>W</code> behavior
without knowing the replication factor. Generally writes will be visible
to subsequent reads when the read consistency level contains enough
nodes to guarantee a quorum intersection with the write consistency
level.</p>
</div>
<div class="paragraph">
<p>The following consistency levels are available:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>ONE</code></dt>
<dd>
<p>Only a single replica must respond.</p>
</dd>
<dt class="hdlist1"><code>TWO</code></dt>
<dd>
<p>Two replicas must respond.</p>
</dd>
<dt class="hdlist1"><code>THREE</code></dt>
<dd>
<p>Three replicas must respond.</p>
</dd>
<dt class="hdlist1"><code>QUORUM</code></dt>
<dd>
<p>A majority (n/2 + 1) of the replicas must respond.</p>
</dd>
<dt class="hdlist1"><code>ALL</code></dt>
<dd>
<p>All of the replicas must respond.</p>
</dd>
<dt class="hdlist1"><code>LOCAL_QUORUM</code></dt>
<dd>
<p>A majority of the replicas in the local datacenter (whichever
datacenter the coordinator is in) must respond.</p>
</dd>
<dt class="hdlist1"><code>EACH_QUORUM</code></dt>
<dd>
<p>A majority of the replicas in each datacenter must respond.</p>
</dd>
<dt class="hdlist1"><code>LOCAL_ONE</code></dt>
<dd>
<p>Only a single replica must respond. In a multi-datacenter cluster,
this also gaurantees that read requests are not sent to replicas in a
remote datacenter.</p>
</dd>
<dt class="hdlist1"><code>ANY</code></dt>
<dd>
<p>A single replica may respond, or the coordinator may store a hint. If
a hint is stored, the coordinator will later attempt to replay the
hint and deliver the mutation to the replicas. This consistency level
is only accepted for write operations.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>Write operations <strong>are always sent to all replicas</strong>, regardless of
consistency level. The consistency level simply controls how many
responses the coordinator waits for before responding to the client.</p>
</div>
<div class="paragraph">
<p>For read operations, the coordinator generally only issues read commands
to enough replicas to satisfy the consistency level. The one exception
to this is when speculative retry may issue a redundant read request to
an extra replica if the original replicas have not responded within a
specified time window.</p>
</div>
<div class="sect3">
<h4 id="picking-consistency-levels"><a class="anchor" href="#picking-consistency-levels"></a>Picking Consistency Levels</h4>
<div class="paragraph">
<p>It is common to pick read and write consistency levels such that the
replica sets overlap, resulting in all acknowledged writes being visible
to subsequent reads. This is typically expressed in the same terms
Dynamo does, in that <code>W + R &gt; RF</code>, where <code>W</code> is the write consistency
level, <code>R</code> is the read consistency level, and <code>RF</code> is the replication
factor. For example, if <code>RF = 3</code>, a <code>QUORUM</code> request will require
responses from at least <code>2/3</code> replicas. If <code>QUORUM</code> is used for both
writes and reads, at least one of the replicas is guaranteed to
participate in <em>both</em> the write and the read request, which in turn
guarantees that the quorums will overlap and the write will be visible
to the read.</p>
</div>
<div class="paragraph">
<p>In a multi-datacenter environment, <code>LOCAL_QUORUM</code> can be used to provide
a weaker but still useful guarantee: reads are guaranteed to see the
latest write from within the same datacenter. This is often sufficient
as clients homed to a single datacenter will read their own writes.</p>
</div>
<div class="paragraph">
<p>If this type of strong consistency isn&#8217;t required, lower consistency
levels like <code>LOCAL_ONE</code> or <code>ONE</code> may be used to improve throughput,
latency, and availability. With replication spanning multiple
datacenters, <code>LOCAL_ONE</code> is typically less available than <code>ONE</code> but is
faster as a rule. Indeed <code>ONE</code> will succeed if a single replica is
available in any datacenter.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="distributed-cluster-membership-and-failure-detection"><a class="anchor" href="#distributed-cluster-membership-and-failure-detection"></a>Distributed Cluster Membership and Failure Detection</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The replication protocols and dataset partitioning rely on knowing which
nodes are alive and dead in the cluster so that write and read
operations can be optimally routed. In Cassandra liveness information is
shared in a distributed fashion through a failure detection mechanism
based on a gossip protocol.</p>
</div>
<div class="sect2">
<h3 id="gossip"><a class="anchor" href="#gossip"></a>Gossip</h3>
<div class="paragraph">
<p>Gossip is how Cassandra propagates basic cluster bootstrapping
information such as endpoint membership and internode network protocol
versions. In Cassandra&#8217;s gossip system, nodes exchange state information
not only about themselves but also about other nodes they know about.
This information is versioned with a vector clock of
<code>(generation, version)</code> tuples, where the generation is a monotonic
timestamp and version is a logical clock the increments roughly every
second. These logical clocks allow Cassandra gossip to ignore old
versions of cluster state just by inspecting the logical clocks
presented with gossip messages.</p>
</div>
<div class="paragraph">
<p>Every node in the Cassandra cluster runs the gossip task independently
and periodically. Every second, every node in the cluster:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Updates the local node&#8217;s heartbeat state (the version) and constructs
the node&#8217;s local view of the cluster gossip endpoint state.</p>
</li>
<li>
<p>Picks a random other node in the cluster to exchange gossip endpoint
state with.</p>
</li>
<li>
<p>Probabilistically attempts to gossip with any unreachable nodes (if
one exists)</p>
</li>
<li>
<p>Gossips with a seed node if that didn&#8217;t happen in step 2.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>When an operator first bootstraps a Cassandra cluster they designate
certain nodes as "seed" nodes. Any node can be a seed node and the only
difference between seed and non-seed nodes is seed nodes are allowed to
bootstrap into the ring without seeing any other seed nodes.
Furthermore, once a cluster is bootstrapped, seed nodes become
"hotspots" for gossip due to step 4 above.</p>
</div>
<div class="paragraph">
<p>As non-seed nodes must be able to contact at least one seed node in
order to bootstrap into the cluster, it is common to include multiple
seed nodes, often one for each rack or datacenter. Seed nodes are often
chosen using existing off-the-shelf service discovery mechanisms.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Note</div>
<div class="paragraph">
<p>Nodes do not have to agree on the seed nodes, and indeed once a cluster
is bootstrapped, newly launched nodes can be configured to use any
existing nodes as "seeds". The only advantage to picking the same nodes
as seeds is it increases their usefullness as gossip hotspots.
====Currently, gossip also propagates token metadata and schema
<em>version</em> information. This information forms the control plane for
scheduling data movements and schema pulls. For example, if a node sees
a mismatch in schema version in gossip state, it will schedule a schema
sync task with the other nodes. As token information propagates via
gossip it is also the control plane for teaching nodes which endpoints
own what data.</p>
</div>
<div class="paragraph">
<p>=== Ring Membership and Failure Detection</p>
</div>
<div class="paragraph">
<p>Gossip forms the basis of ring membership, but the <strong>failure detector</strong>
ultimately makes decisions about if nodes are <code>UP</code> or <code>DOWN</code>. Every node
in Cassandra runs a variant of the
<a href="https://www.computer.org/csdl/proceedings-article/srds/2004/22390066/12OmNvT2phv">Phi
Accrual Failure Detector</a>, in which every node is constantly making an
independent decision of if their peer nodes are available or not. This
decision is primarily based on received heartbeat state. For example, if
a node does not see an increasing heartbeat from a node for a certain
amount of time, the failure detector "convicts" that node, at which
point Cassandra will stop routing reads to it (writes will typically be
written to hints). If/when the node starts heartbeating again, Cassandra
will try to reach out and connect, and if it can open communication
channels it will mark that node as available.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>UP and DOWN state are local node decisions and are not propagated with
gossip. Heartbeat state is propagated with gossip, but nodes will not
consider each other as "UP" until they can successfully message each
other over an actual network channel.
====Cassandra will never remove a node from gossip state without
explicit instruction from an operator via a decommission operation or a
new node bootstrapping with a <code>replace_address_first_boot</code> option. This
choice is intentional to allow Cassandra nodes to temporarily fail
without causing data to needlessly re-balance. This also helps to
prevent simultaneous range movements, where multiple replicas of a token
range are moving at the same time, which can violate monotonic
consistency and can even cause data loss.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="incremental-scale-out-on-commodity-hardware"><a class="anchor" href="#incremental-scale-out-on-commodity-hardware"></a>Incremental Scale-out on Commodity Hardware</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Cassandra scales-out to meet the requirements of growth in data size and
request rates. Scaling-out means adding additional nodes to the ring,
and every additional node brings linear improvements in compute and
storage. In contrast, scaling-up implies adding more capacity to the
existing database nodes. Cassandra is also capable of scale-up, and in
certain environments it may be preferable depending on the deployment.
Cassandra gives operators the flexibility to chose either scale-out or
scale-up.</p>
</div>
<div class="paragraph">
<p>One key aspect of Dynamo that Cassandra follows is to attempt to run on
commodity hardware, and many engineering choices are made under this
assumption. For example, Cassandra assumes nodes can fail at any time,
auto-tunes to make the best use of CPU and memory resources available
and makes heavy use of advanced compression and caching techniques to
get the most storage out of limited memory and storage capabilities.</p>
</div>
<div class="sect2">
<h3 id="simple-query-model"><a class="anchor" href="#simple-query-model"></a>Simple Query Model</h3>
<div class="paragraph">
<p>Cassandra, like Dynamo, chooses not to provide cross-partition
transactions that are common in SQL Relational Database Management
Systems (RDBMS). This both gives the programmer a simpler read and write
API, and allows Cassandra to more easily scale horizontally since
multi-partition transactions spanning multiple nodes are notoriously
difficult to implement and typically very latent.</p>
</div>
<div class="paragraph">
<p>Instead, Cassanda chooses to offer fast, consistent, latency at any
scale for single partition operations, allowing retrieval of entire
partitions or only subsets of partitions based on primary key filters.
Furthermore, Cassandra does support single partition compare and swap
functionality via the lightweight transaction CQL API.</p>
</div>
</div>
<div class="sect2">
<h3 id="simple-interface-for-storing-records"><a class="anchor" href="#simple-interface-for-storing-records"></a>Simple Interface for Storing Records</h3>
<div class="paragraph">
<p>Cassandra, in a slight departure from Dynamo, chooses a storage
interface that is more sophisticated then "simple key value" stores but
significantly less complex than SQL relational data models. Cassandra
presents a wide-column store interface, where partitions of data contain
multiple rows, each of which contains a flexible set of individually
typed columns. Every row is uniquely identified by the partition key and
one or more clustering keys, and every row can have as many columns as
needed.</p>
</div>
<div class="paragraph">
<p>This allows users to flexibly add new columns to existing datasets as
new requirements surface. Schema changes involve only metadata changes
and run fully concurrently with live workloads. Therefore, users can
safely add columns to existing Cassandra databases while remaining
confident that query performance will not degrade.</p>
</div>
</div>
</div>
</div>
</article>
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
  </div>
</main>
</div>
<footer class="footer">
  <p>This page was built using the Antora default UI.</p>
  <p>The source code for this UI is licensed under the terms of the MPL-2.0 license.</p>
</footer>
<script src="../../../../_/js/site.js"></script>
<script async src="../../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
